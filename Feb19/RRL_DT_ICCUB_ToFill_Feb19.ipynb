{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is a notebook to run a simple binary classification algorithm, using Decision Trees.\n",
    "\n",
    "# Author: Viviana Acquaviva\n",
    "# License: BSD but really should be TBD - just be nice.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "#from sklearn.externals.six import StringIO\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Notes: \n",
    "\n",
    "# Data come from here\n",
    "#from astroML.datasets import fetch_rrlyrae_combined\n",
    "#X, y = fetch_rrlyrae_combined()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ok, now time to get real!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The previous data set was just a small/curated selection of the total, which is the one below. Let's read it in:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Xbig = pd.read_csv('RRLyrae_features.txt', names = ['u-g', 'g-r', 'r-i', 'i-z'])\n",
    "Ybig = pd.read_csv('RRLyrae_labels.txt', header = None).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot ALL the data, ahem!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(Xbig['u-g'], Xbig['r-i'], \\\n",
    "            c = Ybig.iloc[:,0].values, marker = '*', s =20, label = None, cmap = 'brg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's take a look at how many positive examples (variable stars) we have."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Number of positive examples:', Ybig.sum().values[0]) \n",
    "print('Total number of examples:', Ybig.size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's do some data thinking.\n",
    "\n",
    "<br>\n",
    "What is noticeable about this data set?\n",
    "\n",
    "Do you expect a decision tree to be an optimal classifier, based on the shape of the data?\n",
    "\n",
    "How would a classifier that puts everything in the \"non-RR Lyrae\" box fare on this data set?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Answers go here :) \n",
    "\n",
    "# What is noticeable about this data set?\n",
    "# Data is imbalanced!\n",
    "\n",
    "# Do you expect a decision tree to be an optimal classifier, based on the shape of the data?\n",
    "# Probably not, with current features\n",
    "\n",
    "# How would a classifier that puts everything in the \"non-RR Lyrae\" box fare on this data set?\n",
    "# Accuracy would be (93141-483)/93141 -> pretty close to 1; recall would be 0; precision would be undefined :( \n",
    "\n",
    "number_of_normal_stars = Ybig.size-Ybig.sum().values[0]\n",
    "number_of_variable_stars = Ybig.sum().values[0]\n",
    "\n",
    "TP = 0.\n",
    "FP = 0.\n",
    "TN = number_of_normal_stars\n",
    "FN = number_of_variable_stars\n",
    "\n",
    "# Accuracy \n",
    "# Very close to 1\n",
    "print('Accuracy:', (TP+TN)/(TP+FP+TN+FN))\n",
    "\n",
    "# Recall\n",
    "# Zero (I think)\n",
    "print('Recall:', TP/(TP+FN))\n",
    "\n",
    "# Precision\n",
    "# Undefined (0/0)\n",
    "if (TP+FP == 0.):\n",
    "    print('Precision: Undefined')\n",
    "else:\n",
    "    print('Precision:', TP/(TP+FP))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at how our previous algorithm would fare on this data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The numbers here are taken from the example with a small data set from last week\n",
    "plt.scatter(Xbig['u-g'], Xbig['r-i'], \\\n",
    "            c = Ybig.iloc[:,0].values, marker = '*', s =20, label = None, cmap = 'brg')\n",
    "plt.axvline(x=0.218, linewidth =1, label = '1st split')\n",
    "plt.axvline(x=0.147, linewidth =1, ls = '--', label = '2nd split')\n",
    "plt.axhline(y=0.035, linewidth =1, ls = '-.', xmin = 0.53, xmax=0.65, label = '3rd split')\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: How is our old tree doing? What is it getting right and wrong?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Answers go here. \n",
    "# Clearly the model is quite shit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's do our training process again! Here we don't have separate train and test splits so we can create them, we'll call them X_trainb, X_testb etc (for \"big\"). Note: we are not doing cross validation yet, which is bad!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make some splitting into test and training set\n",
    "X_trainb, X_testb, Y_trainb, Y_testb = train_test_split(Xbig, Ybig, random_state=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define and fit the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill this in\n",
    "modelbig = DecisionTreeClassifier()\n",
    "modelbig.fit(X_trainb, Y_trainb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the same plotting routine as above to visualize the new tree:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the model\n",
    "dot_data = StringIO()\n",
    "export_graphviz(\n",
    "            modelbig, # note name change\n",
    "            out_file =  dot_data,\n",
    "            feature_names = list(X_trainb.columns), # here too\n",
    "            class_names = ['Not var','Var'],\n",
    "            filled = True,\n",
    "rounded = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's take a look at those colors and then evaluate how the tree is doing on the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill this in\n",
    "\n",
    "# 100% accuracy, but goes down to very far leaves"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the performance on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill this in\n",
    "\n",
    "# Make the model predictions for the test data\n",
    "Y_pred = modelbig.predict(X_testb)\n",
    "\n",
    "# Calculate the metrics\n",
    "print('Accuracy:', metrics.accuracy_score(Y_testb, Y_pred))\n",
    "print('Recall:', metrics.recall_score(Y_testb,Y_pred))\n",
    "print('Precision:', metrics.precision_score(Y_testb, Y_pred))\n",
    "print()\n",
    "\n",
    "# Full confusion matrix\n",
    "print('Confusion matrix: [[TN, FP], [FN, TP]]')\n",
    "print(metrics.confusion_matrix(Y_testb, Y_pred))\n",
    "print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recap: what have we seen so far?\n",
    "\n",
    "Let's talk about what we should be doing to optimize this classifier, going back to the tools we mentioned."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ideas here\n",
    "\n",
    "# 1. Balance the data sets; limit the depth\n",
    "\n",
    "# 2. k fold cross validation\n",
    "\n",
    "# 3. Check that we have enough data\n",
    "\n",
    "# 4. Engineer new features\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Look at the properties of the first attempt\n",
    "modelbig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can customize this cell as we try new models (changing max_depth)\n",
    "modelX = DecisionTreeClassifier(max_depth=5, random_state=5)\n",
    "modelX.fit(X_trainb, Y_trainb)\n",
    "dot_data = StringIO()\n",
    "export_graphviz(\n",
    "            modelX,\n",
    "            out_file =  dot_data,\n",
    "            feature_names = list(X_trainb.columns),\n",
    "            class_names = ['Not var','Var'],\n",
    "            filled = True,\n",
    "rounded = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And then look at some of these to see what is happening\n",
    "\n",
    "print('Training metrics')\n",
    "print('Accuracy:', metrics.accuracy_score(Y_trainb, modelX.predict(X_trainb)))\n",
    "print('Precision:', metrics.precision_score(Y_trainb, modelX.predict(X_trainb)))\n",
    "print('Recall:', metrics.recall_score(Y_trainb, modelX.predict(X_trainb)))\n",
    "print()\n",
    "\n",
    "print('Test metrics')\n",
    "print('Accuracy:', metrics.accuracy_score(Y_testb, modelX.predict(X_testb)))\n",
    "print('Precision:', metrics.precision_score(Y_testb, modelX.predict(X_testb)))\n",
    "print('Recall:', metrics.recall_score(Y_testb, modelX.predict(X_testb)))\n",
    "print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can customize this cell as we try new models (changing the weight and max_depth)\n",
    "modelX = DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=5)\n",
    "modelX.fit(X_trainb, Y_trainb)\n",
    "dot_data = StringIO()\n",
    "export_graphviz(\n",
    "            modelX,\n",
    "            out_file =  dot_data,\n",
    "            feature_names = list(X_trainb.columns),\n",
    "            class_names = ['Not var','Var'],\n",
    "            filled = True,\n",
    "rounded = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And then look at some of these to see what is happening\n",
    "\n",
    "print('Training metrics')\n",
    "print('Accuracy:', metrics.accuracy_score(Y_trainb, modelX.predict(X_trainb)))\n",
    "print('Precision:', metrics.precision_score(Y_trainb, modelX.predict(X_trainb)))\n",
    "print('Recall:', metrics.recall_score(Y_trainb, modelX.predict(X_trainb)))\n",
    "print()\n",
    "\n",
    "print('Test metrics')\n",
    "print('Accuracy:', metrics.accuracy_score(Y_testb, modelX.predict(X_testb)))\n",
    "print('Precision:', metrics.precision_score(Y_testb, modelX.predict(X_testb)))\n",
    "print('Recall:', metrics.recall_score(Y_testb, modelX.predict(X_testb)))\n",
    "print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  How to do k-fold cross validation: cross_val_score and cross_val_predict (and cross_validate)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importing some useful stuff\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with a simple K fold with 10 splits."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CV stands for cross validation here\n",
    "cv = KFold(n_splits=10) \n",
    "\n",
    "# This does a lot of things\n",
    "scores = cross_val_score(modelX, Xbig, Ybig, cv=cv, scoring='recall')\n",
    "\n",
    "print('All scores:', scores)\n",
    "print()\n",
    "\n",
    "print('Mean score:', scores.mean())\n",
    "print('Score standard deviation:', scores.std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What happened?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill\n",
    "\n",
    "# You need to define shuffle=True because all the variable stars (value 1 in the table) are at the end of the table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can try again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CV is cross-validation strategy\n",
    "cv = KFold(n_splits = 10, shuffle=True) #Fill\n",
    "\n",
    "scores = cross_val_score(modelX, Xbig, Ybig, cv=cv, scoring ='recall')\n",
    "\n",
    "print('All scores:', scores)\n",
    "print('Mean score:', scores.mean())\n",
    "print('Score standard deviation:', scores.std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### For severely imbalanced classifiers, stratification (the process of conserving the class membership distribution in each fold) might help. Let's stratify and shuffle (although TBH stratification does shuffle!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CV is cross-validation strategy\n",
    "# Statified here means that class membership is preserved\n",
    "cv = StratifiedKFold(shuffle=True, n_splits=10)\n",
    "\n",
    "scores = cross_val_score(modelX, Xbig, Ybig, cv = cv, scoring = 'recall')\n",
    "\n",
    "print('All scores:', scores)\n",
    "print('Mean score:', scores.mean())\n",
    "print('Score standard deviation:', scores.std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are curious about Stratified K Fold vs K Fold..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the cross validation... thing\n",
    "cv = KFold(shuffle = True, n_splits = 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We see here that the test samples have different numbers of members\n",
    "for train, test in cv.split(Xbig, Ybig):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(Ybig.values.ravel()[train]), np.bincount(Ybig.values.ravel()[test])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the cross validation... thing\n",
    "cv = StratifiedKFold(shuffle=True, n_splits=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We see here that the test samples have almost the same number of members +/- 1\n",
    "for train, test in cv.split(Xbig, Ybig):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "    np.bincount(Ybig.values.ravel()[train]), np.bincount(Ybig.values.ravel()[test])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How can we find the confusion matrix in a cross-validation scheme?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One possibility is to use the cross_val_predict utility function, which gives one realization obtained by combining the predictions on each of the test folds (should not be used for evaluation because it does not have uncertainty.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make predicitons by combining teach test fold\n",
    "Y_pred = cross_val_predict(modelX, Xbig, Ybig, cv=cv) #No scoring parameter here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make and show the confusion matrix\n",
    "cm = metrics.confusion_matrix(Ybig, Y_pred)\n",
    "print(cm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Helper function to make a nice confusion matrix plot\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "#    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                 color=\"green\" if i == j else \"red\", fontsize = 30)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show a neat confusion matrix\n",
    "plot_confusion_matrix(cm, classes = ['Normal', 'Variable'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A word about the cross_validate function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function cross_validate is nice because it allows you to check train/test scores within the cross validation process (while so far we had only done this on a train/test split)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_validate(modelX, Xbig, Ybig, scoring= 'recall', cv = cv, return_train_score=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "They give a view of the train/test gap and allow us to check whether more data would help."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This helps visualize the training vs the test. \n",
    "# Will help us visualize and see what would happen if we use more data or if the data has plateaued. \n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), scoring = 'accuracy'):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    X= scaler.fit_transform(X)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation test score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv = StratifiedKFold(n_splits=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_learning_curve(modelX, 'Learning Curves', Xbig, Ybig,  cv = cv, scoring = 'recall');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is happening up there?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Some non shuffling is being a problem here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Useful shuffling routines\n",
    "from sklearn.utils import shuffle\n",
    "XbigS, ybigS = shuffle(Xbig, Ybig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make a learning curve with the new shuffled data\n",
    "plot_learning_curve(modelX, 'Learning Curves', XbigS, ybigS,  cv = cv, scoring = 'recall')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusions?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# None"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('shims': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "ac44d8818409a3fb5504ad5aea037bd15272a8155f7b0a2c4e15bdda4027f435"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}